########################################################################
#                                                                      #
#   Cyprium is a multifunction cryptographic, steganographic and       #
#   cryptanalysis tool developped by members of The Hackademy.         #
#   French White Hat Hackers Community!                                #
#   www.thehackademy.fr                                                #
#   Copyright © 2012                                                   #
#   Authors: SAKAROV, Madhatter, mont29, Luxerails, PauseKawa, fred,   #
#   afranck64, Tyrtamos.                                               #
#   Contact: cyprium@thehackademy.fr, sakarov@thehackademy.fr,         #
#   madhatter@thehackademy.fr, mont29@thehackademy.fr,                 #
#   irc.thehackademy.fr #cyprium, irc.thehackademy.fr #hackademy       #
#                                                                      #
#   Cyprium is free software: you can redistribute it and/or modify    #
#   it under the terms of the GNU General Public License as published  #
#   by the Free Software Foundation, either version 3 of the License,  #
#   or any later version.                                              #
#                                                                      #
#   This program is distributed in the hope that it will be useful,    #
#   but without any warranty; without even the implied warranty of     #
#   merchantability or fitness for a particular purpose. See the       #
#   GNU General Public License for more details.                       #
#                                                                      #
#   The terms of the GNU General Public License is detailed in the     #
#   COPYING attached file. If not, see : http://www.gnu.org/licenses   #
#                                                                      #
########################################################################


import sys
import itertools
import time
import string
import pickle

import kernel.cache as cache
import settings


DO_CACHE = settings.CCH_USE


class MatchDic(object):
    """
    That class can create seleveral lists of words by transforming them
    from those generated e.g. by an Hunspell dic, and applying to them
    some operations to get only given charset-compatible chars.

    Then, it can check some text against those list, and return the matching
    level (from 0.0, no matching, to 1.0, full matching – usually, > 0.8
    means match!).
    """

    # Used to ensure cached data was cached with same version.
    # Change that when modifying parse code!
    _hash_salt = b"1.0.0"

    def __init__(self, word_gen):
        """
        word_gen must implement three things:
            * An ids member returning a list of ids of all dics it handles.
            * A get_hash function, which must return an hash object (as
              generated by haslib module) for given uid.
            * A gen_words function, taking a list of ids as parameter, and
              returning a list (or better, a generator) of words.
        """
        self.ids = {}
        self.word_gen = word_gen

    def preprocess(self, txt):
        """
        Applies text pre-processing, as explained in init() doc.
        Returns None in case the text should be discarded.
        """
        if self.charmap and self.charset:
            txt = "".join(c for c in txt \
                            if c in self.charset).translate(self.charmap)
        elif self.charset:
            txt = "".join(c for c in txt if c in self.charset)
        elif self.charmap:
            txt = txt.translate(self.charmap)
        if not self.do_minmax_len or self.minlen <= len(txt) < self.maxlen:
            return txt
        # else return None.

    def init(self, ids=None, charset=None, charmap=None, minlen=None,
             maxlen=None):
        """
        Inits this MatchDic object, by getting all words generated from set
        generator for the given ids (or all, if None).
        It will optionnaly apply to each generated word, before storing it:
            * charset and charmap operations (i.e. removing from all char
              not in charset, and then calling str.translate with charmap).
            * length operation (i.e. rejecting words smaller than minlen
              (defaults to 1) or longer than maxlen (defaults to 32767).
        """
        if charset:
            self.charset = set(charset)
        else:
            self.charset = None
        if charmap:
            self.charmap = str.maketrans(charmap)
        else:
            self.charmap = None
        if minlen or maxlen:
            if not minlen:
                minlen = 1
            if not maxlen:
                maxlen = 32767  # XXX Arbitrary high value.
        self.minlen = minlen
        self.maxlen = maxlen
        self.do_minmax_len = bool(minlen or maxlen)
        if DO_CACHE:
            hsh_param = pickle.dumps((self.charset, self.charmap,
                                      minlen, maxlen))

        if ids == None:
            ids = self.word_gen.ids
        for uid in ids:
            if DO_CACHE:
                hsh = self.word_gen.get_hash(uid)
                hsh.update(hsh_param)
                hsh.update(self._hash_salt)
                hsh = hsh.hexdigest()
                if hsh in cache.cache:
                    self.ids[uid] = cache.cache.get(hsh)
                    continue
            lst = self.ids[uid] = []
            lst_ln = len(lst)
            for w in (self.preprocess(w) for w in self.word_gen.gen_words(dics=(uid,))):
                if not w:
                    continue
                ln = len(w)
                if ln > lst_ln:
                    lst += [set() for i in range(ln - lst_ln)]
                    lst_ln = ln
                lst[ln - 1].add(w)
            if DO_CACHE:
                cache.cache.cache(hsh, lst)

    def get_match_level(self, uid, text):
        """
        Returns a float value (in [0.0, 1.0]), the highest it is, the better
        the given text match content (words) of the given uid's dic.
        """
        chunks = [self.preprocess(w) for w in text.split()]
        scoria = []
        brk = False
        while chunks:
            chk = chunks.pop(0)
            if not chk:
                continue
            # Check from longest words to shortest!
            # However, no need to test words longer than chunk!
            ln = len(chk)
            width = min(ln - 1, len(self.ids[uid]) - 1)
            # Simple obvious test!
            if chk in self.ids[uid][width]:
                continue
            # Else, for each possible word length, use a sliding window 
            # over the chunk to search for a matching word (starting from
            # longest ones).
            for wlst in reversed(self.ids[uid][:width]):
                for i in range(0, ln - width):
                    if chk[i:i + width] in wlst:
                        chunks += (chk[:i], chk[i + width:])
                        brk = True
                        break
                if brk:
                    break
                width -= 1
            if not brk:  # No match where found for this chunk.
                scoria.append(chk)
            else:
                brk = False
        return (len(text) - len("".join(scoria))) / len(text)

    def find_best_dic(self, text, ):
        """
        Returns a dict {uid: value} containing all loaded dics’ uid, and their
        matching value against given text.
        Note: For performance reasons, you should feed it with small chunks of
              text – usually, 50 to 100 chars are enough!
        """
        res = {}
        for uid in self.ids.keys():
            res[uid] = self.get_match_level(uid, text)
        return res
